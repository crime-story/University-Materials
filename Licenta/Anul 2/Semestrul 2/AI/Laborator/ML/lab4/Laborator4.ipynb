{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z6onTYgkmmix"
   },
   "source": [
    "### Exercitiul 2\n",
    "\n",
    "Pentru acest exercitui am implementat 2 variante ale metodei de normalizare.\n",
    "\n",
    "Prima metoda foloseste libraria numpy si a 2-a foloseste scikit-learn.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R4PfWgQHOAQO"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def normalize_data(train_data, test_data, type=None):\n",
    "  if type =='standard': \n",
    "    # standardizarea e obtinuta prin scaderea mediilor atributelor si impartirea la deviatiile standard.\n",
    "    # Mediile si deviatiile standard sunt obtinute din datele de antrenare\n",
    "    mean = np.mean(train_data,axis=0) # calculeaza mediile atributelor din datele de antrenare\n",
    "    std = np.std(train_data,axis=0) # calculeaza deviatiile standard din datele de antrenare\n",
    "    # Mean si std sunt 2 vectori de dimensiune egala cu numarul de atribute(features) din setul de date.\n",
    "    # In cazul acestui laborator atributele sunt frecventele de cuvinte.\n",
    "    train_data -= mean\n",
    "    train_data/=std\n",
    "    test_data -= mean\n",
    "    test_data /=std\n",
    "  elif type == 'l1':\n",
    "    train_data = train_data / (np.expand_dims((np.linalg.norm(train_data,ord=1,axis=1)), axis=1) + 1e-6) # normalizarea L1.\n",
    "    # 1e-6 este adunat pt cazurile in care norma e 0. In cazul acestui laborator, norma poate fi 0 daca un document nu are niciun cuvant\n",
    "    # sau cuvintele pe care le contine nu sunt gasite in vocabular (acest al 2-lea caz fiind posibil pentru datele de test)\n",
    "    # Iar np.expand_dims e folosit pentru a putea folosi broadcast(primul laborator pentru detalii) la impartire.\n",
    "    # Mai exact, np.linalg.norm ne intoarce norma 1(pentru ca am setat ord =1) pentru fiecare exemplu din setul de date.\n",
    "    # Deci output-ul este un vector de norme de dimensiune egala cu numarul de exemple din setul de date si pentru a folosi vectorul acesta la impartire e nevoie sa il\n",
    "    # transformam in forma (nr_exemple, 1)\n",
    "    test_data = test_data / (np.expand_dims((np.linalg.norm(test_data, ord=1,axis=1)), axis=1) + 1e-6)\n",
    "  elif type == 'l2':\n",
    "    norm_train = np.expand_dims((np.linalg.norm(train_data,ord=2,axis=1)), axis=1) # identic cu cazul l1, dar parametrul ord este setat la 2, pentru a calcula norma l2\n",
    "    train_data = train_data / (norm_train + 1e-6)\n",
    "    test_data = test_data / (np.expand_dims((np.linalg.norm(test_data, ord=2,axis=1)), axis=1) +1e-6)\n",
    "  return train_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F_OoiyXwPvDN"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, Normalizer\n",
    "def normalize_data_v2(train_data, test_data, type=None):\n",
    "  if type=='standard':\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(train_data)\n",
    "    train_data = scaler.transform(train_data)\n",
    "    test_data = scaler.transform(test_data)\n",
    "  elif type=='l2':\n",
    "    normalizer = Normalizer(norm='l2')\n",
    "    train_data = normalizer.transform(train_data)\n",
    "    test_data = normalizer.transform(test_data)\n",
    "  elif type =='l1':\n",
    "    normalizer = Normalizer(norm='l1')\n",
    "    train_data = normalizer.transform(train_data)\n",
    "    test_data = normalizer.transform(test_data)\n",
    "  return train_data, test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RfUzsMfboh29"
   },
   "source": [
    "### Exercitiul 3 si 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w3Si9dIERVFq"
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "class BagOfWords:\n",
    "\n",
    "  def __init__(self):\n",
    "    self.vocab = {}\n",
    "    self.words = []\n",
    "\n",
    "  def build_vocabulary(self, data):\n",
    "    # pentru fiecare cuvant din variabila \"data\" asociem un id unic,\n",
    "    # care va reprezenta indexul cuvantului in vectorul de features\n",
    "    id_word = 0\n",
    "    for mesaj in data:\n",
    "      for word in mesaj:\n",
    "        if word not in self.vocab:\n",
    "          self.vocab[word]=id_word\n",
    "          id_word+=1\n",
    "          self.words.append(word)\n",
    "  \n",
    "  def get_features(self, data):\n",
    "    # transforma textele din var \"data\" in vectori de dimensiunea vocabularului\n",
    "    # care contin frecventele cuvintelor. Deci feature_matrix este o matrice cu dimensiunile (nr de exemple, nr de cuvinte in vocabular)\n",
    "    # si fiecare rand va contine frecventele cuvintelor pentru un anumit exemplu de propozitie.\n",
    "    # variabila no_word este folosita pentru a afisa exemplele din setul data care nu contin niciun cuvant din vocabular\n",
    "    feature_matrix = np.zeros((data.shape[0],len(self.vocab)))\n",
    "    for i, example in enumerate(data):\n",
    "      no_word = True\n",
    "      for word in example:\n",
    "        if word in self.vocab:\n",
    "          feature_matrix[i,self.vocab[word]] +=1\n",
    "          no_word = False\n",
    "      if no_word:\n",
    "        print(example)\n",
    "    return feature_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hmm8hy-6R9Ds"
   },
   "outputs": [],
   "source": [
    "train_data_original = np.load('./data/training_sentences.npy',allow_pickle=True)\n",
    "train_labels = np.load('./data/training_labels.npy')\n",
    "test_data_original = np.load('./data/test_sentences.npy',allow_pickle=True)\n",
    "test_labels = np.load('./data/test_labels.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2xjk4NBFUhLN",
    "outputId": "18e0eadb-e3a1-4970-95f2-5839f44816c7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9522\n"
     ]
    }
   ],
   "source": [
    "bow = BagOfWords() # cream o instanta a clasei BagOfWords\n",
    "bow.build_vocabulary(train_data_original) # construim vocabularul folosind datele de antrenare. \n",
    "print(len(bow.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RF601eKMZTfo"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u1BAJoWRpkrd"
   },
   "source": [
    "### Exercitiul 5\n",
    "\n",
    "Urmatoarele 2 celule reprezinta ex5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pex7wszMx8r4"
   },
   "source": [
    "Urmatoarea celula transforma datele din propozitii in vectori de frecventa. Iar exemplele pe care le printeaza sunt exemplele care nu contin niciun cuvant in vocabular-ul creat anterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ki4jnV_eUPtG",
    "outputId": "28685ce1-b613-4a6e-bda7-6247bd6bb409"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[]\n",
      "['Ultimately', 'tor', 'motive', 'tui', 'achieve', 'korli']\n",
      "['22', '146tf150p']\n",
      "['YO', 'YO', 'YO', 'BYATCH', 'WHASSUP']\n",
      "['Ryder', 'unsoldnow', 'gibbs']\n",
      "['Kkcongratulation']\n",
      "['hanks', 'lotsly']\n",
      "['645']\n"
     ]
    }
   ],
   "source": [
    "train_data_original = bow.get_features(train_data_original)\n",
    "test_data_original = bow.get_features(test_data_original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a8GKkwdlU0_N"
   },
   "outputs": [],
   "source": [
    "train_data, test_data = normalize_data_v2(train_data_original,test_data_original,type='l2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ivFq2KA_pu1g"
   },
   "source": [
    "### Exercitiul 6\n",
    "\n",
    "Pentru antrenarea modelului folosim functia fit.\n",
    "\n",
    "Iar cu functia predict obtinem predictiile pentru setul de test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fApYd7IHUZcY",
    "outputId": "fafc8a42-3b07-485d-d42c-5a9a59981e02"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1, kernel='linear')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "svm_model = svm.SVC(C=1,kernel= \"linear\") # definirea modelului\n",
    "svm_model.fit(train_data, train_labels) # antrenare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ozJpCY3_U-ml",
    "outputId": "cc53628b-0ad6-4679-b12f-fbefb4b7c9a6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9842391304347826 F1-score 0.9409368635437881\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "\n",
    "pred_labels = svm_model.predict(test_data) # ne intoarce predictiile pentru fiecare exemplu din test_data\n",
    "print(\"Accuracy:\", accuracy_score(test_labels,pred_labels),\"F1-score:\", f1_score(test_labels, pred_labels)) # acuratetea si scorul F1 se calculeaza din\n",
    "                                                                                                            # predictii (pred_labels) si clasele reale(test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tkwQhB3UqfXH"
   },
   "source": [
    "Mesajele spam au label-ul 1, iar cele non-spam au label-ul 0 (pentru noi 0 inseamna de fapt -1, asa categorisam exemplele negative). \n",
    "\n",
    "Functia de decizie a svm-ului avea forma wx+b = w1x1+w2x2+ ... + wnxn + b. Si spuneam ca predictia este 1(spam) daca wx+b este mai mare decat 0 si -1(non-spam) daca valoarea functiei este mai mica decat 0. Asta inseamna ca w-urile cu valorile ridicate corespund cuvintelor importante pentru predictia 1 (spam), iar w-urile cu valori mici(chiar negative) corespund cuvintelor care favorizeaza predictia -1(non-spam). Deci pentru a gasi cuvintele cele mai negative, ne uitam la coeficientii w si luam cuvintele care corespund unor w-uri mari (adica favorizeaza spam). Si invers pentru cuvintele cele mai pozitive, ne uitam la coeficientii w si luam cuvintele care corespund unor w-uri mici (adica favorizeaza non-spam).\n",
    "\n",
    "Coeficientii(w) ii accesam cu atributul coef_[0], si putem folosi functia argsort din numpy pentru a lua indecsii (care corespund si cuvintelor) in ordinea crescatoare a coeficientilor. Apoi primele 10 si ultimele 10 cuvinte din vocabular (in ordinea inserarii lor) sunt cuvintele cerute de exercitiu.\n",
    "\n",
    "\n",
    "\n",
    "#### Observatie:\n",
    " Acest rationament este valabil pentru ca folosim pentru svm kernel-ul 'linear', daca foloseam 'rbf' sau altceva era invalid deoarece functia de decizie nu mai avea aceeasi interpretabilitate.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "71tAUeS1rDCR",
    "outputId": "75164e24-203f-4df2-f8b5-ba1d9c70cd21"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most positive words: ['&lt#&gt' 'me' 'i' 'Going' 'him' 'Ok' 'I' 'Ill' 'my' 'Im']\n",
      "Most negative words: ['Text' 'To' 'mobile' 'CALL' 'FREE' 'txt' '&' 'Call' 'Txt' 'STOP']\n"
     ]
    }
   ],
   "source": [
    "index_sort = np.argsort(svm_model.coef_[0])\n",
    "bow.words = np.array(bow.words)\n",
    "print(\"Most positive words:\", bow.words[index_sort[:10]])\n",
    "print(\"Most negative words:\", bow.words[index_sort[-10:]])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dQpw8rpPSttp"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uLlINPb3PrMj",
    "outputId": "2c2200e0-b0cf-467d-a3db-935bded36b5f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  ./data_lab5.zip\n",
      "  inflating: data/test_labels.npy    \n",
      "  inflating: data/test_sentences.npy  \n",
      "  inflating: data/training_labels.npy  \n",
      "  inflating: data/training_sentences.npy  \n"
     ]
    }
   ],
   "source": [
    "!unzip ./data_lab5.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mF8XK9OARUOt"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Laborator4.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
